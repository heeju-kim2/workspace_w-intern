wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.8.10
    cli_version: 0.17.3
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1719919434
    t:
      1:
      - 1
      - 5
      - 11
      - 49
      - 51
      - 53
      - 55
      - 71
      - 98
      - 100
      - 105
      2:
      - 1
      - 5
      - 11
      - 49
      - 51
      - 53
      - 55
      - 71
      - 98
      - 100
      - 105
      3:
      - 23
      4: 3.8.10
      5: 0.17.3
      6: 4.41.2
      8:
      - 5
      13: linux-x86_64
model_name:
  desc: null
  value: ../models/Llama-2-7b-chat-hf
tokenizer_name:
  desc: null
  value: null
enable_fsdp:
  desc: null
  value: true
low_cpu_fsdp:
  desc: null
  value: false
run_validation:
  desc: null
  value: true
batch_size_training:
  desc: null
  value: 1
batching_strategy:
  desc: null
  value: packing
context_length:
  desc: null
  value: 4096
gradient_accumulation_steps:
  desc: null
  value: 4
gradient_clipping:
  desc: null
  value: false
gradient_clipping_threshold:
  desc: null
  value: 1.0
num_epochs:
  desc: null
  value: 3
max_train_step:
  desc: null
  value: 0
max_eval_step:
  desc: null
  value: 0
num_workers_dataloader:
  desc: null
  value: 1
lr:
  desc: null
  value: 0.0001
weight_decay:
  desc: null
  value: 0.0
gamma:
  desc: null
  value: 0.85
seed:
  desc: null
  value: 42
use_fp16:
  desc: null
  value: false
mixed_precision:
  desc: null
  value: true
val_batch_size:
  desc: null
  value: 1
peft_method:
  desc: null
  value: lora
use_peft:
  desc: null
  value: true
from_peft_checkpoint:
  desc: null
  value: ''
output_dir:
  desc: null
  value: ../samsum_outputs/Llama-2-7b-chat-hf/mixed/ep_3_lr_1e-4_bs_4_wd_0.0
freeze_layers:
  desc: null
  value: false
num_freeze_layers:
  desc: null
  value: 1
quantization:
  desc: null
  value: false
one_gpu:
  desc: null
  value: false
save_model:
  desc: null
  value: true
dist_checkpoint_root_folder:
  desc: null
  value: PATH/to/save/FSDP/model
dist_checkpoint_folder:
  desc: null
  value: fine-tuned
save_optimizer:
  desc: null
  value: false
use_fast_kernels:
  desc: null
  value: false
use_wandb:
  desc: null
  value: true
save_metrics:
  desc: null
  value: false
flop_counter:
  desc: null
  value: false
flop_counter_start:
  desc: null
  value: 3
use_profiler:
  desc: null
  value: false
profiler_dir:
  desc: null
  value: PATH/to/save/profiler/results
dataset:
  desc: null
  value: samsum_dataset
sharding_strategy:
  desc: null
  value: FULL_SHARD
hsdp:
  desc: null
  value: false
sharding_group_size:
  desc: null
  value: 0
replica_group_size:
  desc: null
  value: 0
checkpoint_type:
  desc: null
  value: SHARDED_STATE_DICT
fsdp_activation_checkpointing:
  desc: null
  value: true
fsdp_cpu_offload:
  desc: null
  value: false
pure_bf16:
  desc: null
  value: false
optimizer:
  desc: null
  value: AdamW
peft_type:
  desc: null
  value: LORA
auto_mapping:
  desc: null
  value: null
base_model_name_or_path:
  desc: null
  value: ../models/Llama-2-7b-chat-hf
revision:
  desc: null
  value: null
task_type:
  desc: null
  value: CAUSAL_LM
inference_mode:
  desc: null
  value: false
r:
  desc: null
  value: 8
target_modules:
  desc: null
  value:
  - v_proj
  - q_proj
lora_alpha:
  desc: null
  value: 32
lora_dropout:
  desc: null
  value: 0.05
fan_in_fan_out:
  desc: null
  value: false
bias:
  desc: null
  value: none
use_rslora:
  desc: null
  value: false
modules_to_save:
  desc: null
  value: null
init_lora_weights:
  desc: null
  value: true
layers_to_transform:
  desc: null
  value: null
layers_pattern:
  desc: null
  value: null
rank_pattern:
  desc: null
  value: {}
alpha_pattern:
  desc: null
  value: {}
megatron_config:
  desc: null
  value: null
megatron_core:
  desc: null
  value: megatron.core
loftq_config:
  desc: null
  value: {}
use_dora:
  desc: null
  value: false
layer_replication:
  desc: null
  value: null
